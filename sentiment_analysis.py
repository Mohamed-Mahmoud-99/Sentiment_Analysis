# -*- coding: utf-8 -*-
"""ٍSentiment_Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xnaEWZ-extMCGXq44bV-U6cludPyGHKI
"""

# ------------------------------
# Import Liberaries
# ------------------------------
import numpy as np
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
import matplotlib.pyplot as plt
import nltk
import re


nltk.download('punkt')
nltk.download('punkt_tab')
nltk.download('stopwords')
nltk.download('wordnet')

from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

stop_words = set(stopwords.words('english'))
lemmatizer = WordNetLemmatizer()

# ------------------------------
# Preprocessing
# ------------------------------
def preprocess_text(text):

    text = text.lower()


    text = re.sub(r"[^a-zA-Z\s]", "", text)

    # Tokenization
    tokens = word_tokenize(text)


    tokens = [w for w in tokens if w not in stop_words]

    # Lemmatization
    tokens = [lemmatizer.lemmatize(w) for w in tokens]

    return tokens

# ------------------------------
# Download Dataset
# ------------------------------
def load_imdb_data(train_size=40000, test_size=10000 ,num_words=10000):

    (X_train_full, y_train_full), (X_test_full, y_test_full) = imdb.load_data(num_words=num_words)

    X_train = np.concatenate([X_train_full[:train_size],X_test_full[:15000]])
    y_train = np.concatenate([y_train_full[:train_size],y_test_full[:15000]])


    X_test = X_test_full[15000:]
    y_test = y_test_full[15000:]

    print("len Train: ",len(y_train))
    print("len Test: ",len(y_test))

    print("len Train: ",len(X_train_full))
    print("len Test: ",len(X_test_full))
    return (X_train, y_train), (X_test, y_test)

# ------------------------------
# Prepare Data
# ------------------------------
def prepare_data(X_train, X_test, max_len=250):
    X_train_padded = pad_sequences(X_train, maxlen=max_len)
    X_test_padded = pad_sequences(X_test, maxlen=max_len)
    return X_train_padded, X_test_padded

# ------------------------------
# Build Model
# ------------------------------
def build_lstm_model(input_dim, max_len, embed_dim=265, lstm_units=128, dropout_rate=0.2):
    model = Sequential()
    model.add(Embedding(input_dim=input_dim, output_dim=embed_dim, input_length=max_len))
    model.add(LSTM(lstm_units))
    model.add(Dropout(dropout_rate))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# ------------------------------
# Model Training
# ------------------------------
def train_model(model, X_train, y_train, epochs=5, batch_size=50, validation_split=0.2):
    history = model.fit(
        X_train, y_train,
        epochs=epochs,
        batch_size=batch_size,
        validation_split=validation_split
    )
    return history

# ------------------------------
# Model Evaluation
# ------------------------------
def evaluate_model(model, X_test, y_test):
    loss, accuracy = model.evaluate(X_test, y_test)
    print(f"Test Accuracy: {accuracy*100:.2f}%")
    return loss, accuracy

# ------------------------------
# Drawing Results
# ------------------------------
def plot_history(history):
    plt.plot(history.history['accuracy'], label='train_acc')
    plt.plot(history.history['val_accuracy'], label='val_acc')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

# ------------------------------
# New Sentence
# ------------------------------
def encode_review(text, word_index, num_words=10000, max_len=200):
    tokens = preprocess_text(text)
    encoded = [1]  # START token
    for word in tokens:
        if word in word_index and word_index[word] < num_words:
            encoded.append(word_index[word]+3)
        else:
            encoded.append(2)  # UNK token
    return pad_sequences([encoded], maxlen=max_len)

def predict_sentiment(model, text, word_index):
    encoded_review = encode_review(text, word_index)
    prediction = model.predict(encoded_review)[0][0]
    print(f"Sentiment Score (0=Negative,1=Positive): {prediction:.3f}")
    return prediction

# ------------------------------
# 10️⃣ Main function
# ------------------------------
def main():
    num_words = 20000
    max_len = 250


    (X_train, y_train), (X_test, y_test) = load_imdb_data(num_words=num_words)


    X_train_padded, X_test_padded = prepare_data(X_train, X_test, max_len=max_len)


    model = build_lstm_model(input_dim=num_words, max_len=max_len)


    history = train_model(model, X_train_padded, y_train)


    evaluate_model(model, X_test_padded, y_test)


    plot_history(history)


    word_index = imdb.get_word_index()
    sample_review = "I really loved this movie, it was fantastic!"
    predict_sentiment(model, sample_review, word_index)

# ------------------------------
# Run Project
# ------------------------------
if __name__ == "__main__":
    main()
